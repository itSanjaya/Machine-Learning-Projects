{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0700d54",
   "metadata": {},
   "source": [
    "**Importing Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a974fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a627a35c-de43-4da3-9c95-5a91cad7a315",
   "metadata": {},
   "source": [
    "**Node**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16d5be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, left=None, right=None, threshold=None, feature_index=None, information_gain=None, value=None):\n",
    "        \"\"\"\n",
    "        Initialize a Node object for a decision tree.\n",
    "        \"\"\"\n",
    "        self.left = left             # Set the left child node.\n",
    "        self.right = right             # Set the right child node.\n",
    "        self.threshold = threshold       # Set the threshold value for splitting at this node.\n",
    "        self.feature_index = feature_index    # Set the index of the feature used for splitting at this node.\n",
    "        self.information_gain = information_gain   # Set the information gain of splitting at this node.\n",
    "        self.value = value                       # Set the predicted value at this node (for leaf nodes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9192a05d-e997-43da-b547-cbe39f4fa2b5",
   "metadata": {},
   "source": [
    "**Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66cf4213",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self,min_samples_split=2):\n",
    "        \"\"\"Initialize a DecisionTree object with the specified minimum number of samples required to split a node.\n",
    "        \"\"\"\n",
    "        self.root = None                                  # Initialize the root node as None.\n",
    "        self.min_samples_split = min_samples_split        # Set the minimum number of samples required to split a node.\n",
    "\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        \"\"\"Fit the decision tree to the training data.\"\"\"\n",
    "        \n",
    "        df = np.concatenate((X,y), axis = 1)     # Concatenate the features and labels to create the dataset.\n",
    "        self.root = self.grow_tree(df)         # Build the decision tree using the dataset.\n",
    "\n",
    "        \n",
    "    def grow_tree(self,df):\n",
    "        \"\"\"Recursive function to build the decision tree.\"\"\"\n",
    "        X = df[:,:-1]       # Extract features from the dataframe.\n",
    "        y = df[:,-1]          # Extract labels from the dataframe.\n",
    "        n_samples, n_features = X.shape   # Get the number of samples and features in the dataset.\n",
    "\n",
    "        if (n_samples > self.min_samples_split):\n",
    "            best_one = self.best_split(df,n_samples,n_features)      # Find the best split for the current node.\n",
    "\n",
    "            if (best_one and best_one['information_gain'] > 0):\n",
    "\n",
    "                left_tree = self.grow_tree(best_one['left_part'])     # Recursively build the left subtree.\n",
    "\n",
    "                right_tree = self.grow_tree(best_one['right_part'])       # Recursively build the right subtree.\n",
    "                return Node(left_tree, right_tree, best_one['threshold'], best_one['feature_index'], best_one['information_gain']) # Create a decision node.\n",
    "\n",
    "        leaf_value = self.leaf_val(y)      # Else, Determine the leaf value for the current node.\n",
    "\n",
    "        return Node(value = leaf_value)      # create a leaf node with the determined value.\n",
    "\n",
    "    \n",
    "    def leaf_val(self,y):\n",
    "        \"\"\"Determine the most common label in a set of labels as the leaf value.\"\"\"\n",
    "\n",
    "        return max(list(y), key = list(y).count)\n",
    "\n",
    "    \n",
    "    def best_split(self, df, n_samples, n_features):\n",
    "        \"\"\"Find the best split for the current node.\"\"\"\n",
    "        best_one = {}                # Initialize a dictionary to store information about the best split.\n",
    "        \n",
    "        max_ig = -float('inf')          # Initialize the maximum information gain to negative infinity.\n",
    "        \n",
    "        for index in range(n_features):\n",
    "            \n",
    "            other_val = df[:,index]        # Get the values of the current feature.\n",
    "\n",
    "            vals = set(other_val)          # Get the unique values of the current feature.\n",
    "\n",
    "            for thresh in vals:\n",
    "\n",
    "                left_split,right_split = self.custom_split(df,thresh,index)   # Split the dataset based on the current feature and threshold.\n",
    "\n",
    "                if (len(left_split) > 0 and len(right_split) > 0):\n",
    "                    \n",
    "                    y, y_left, y_right = df[:,-1], left_split[:,-1], right_split[:,-1]       # Extract labels for the splits.\n",
    "                    \n",
    "                    inform_gain = self.calculate_information_gain(y, y_left, y_right)     # Calculate the information gain for the current split.\n",
    "              \n",
    "                    if (inform_gain > max_ig):\n",
    "\n",
    "                        # Update the best split information.\n",
    "                        best_one['feature_index'] = index \n",
    "                        best_one['information_gain'] = inform_gain\n",
    "                        best_one['threshold'] = thresh\n",
    "                        best_one['left_part'] = left_split\n",
    "                        best_one['right_part'] = right_split\n",
    "\n",
    "                        max_ig = inform_gain       # Update the maximum information gain.\n",
    "\n",
    "        return best_one         # Return the information about the best split.\n",
    " \n",
    "    \n",
    "    def custom_split(self, df, thresh, index):\n",
    "        \"\"\"Split the dataset based on a given threshold value for a specific feature.\"\"\"\n",
    "        \n",
    "        feat_val = df[:, index]                   # Get the values of the specified feature.\n",
    "        left_side = df[feat_val <= thresh]        # Create a split for values less than or equal to the threshold.\n",
    "        right_side = df[feat_val > thresh]        # Create a split for values greater than the threshold.\n",
    "        return left_side, right_side              \n",
    "\n",
    "\n",
    "    def entropy(self,y):\n",
    "        \"\"\" Calculate the entropy of a set of labels.\"\"\"\n",
    "        \n",
    "        _, counts = np.unique(y, return_counts=True)               # Get the counts of each unique label.\n",
    "        probabilities = counts / len(y)                              # Calculate the probabilities of each label.\n",
    "        entropy = -np.sum(probabilities * np.log2(probabilities))      # Calculate the entropy.\n",
    "        return entropy\n",
    "    \n",
    "\n",
    "    def calculate_information_gain(self, parent_labels, left_labels, right_labels):\n",
    "        \"\"\"Calculate the information gain of splitting a set of labels into two subsets.\"\"\"\n",
    "\n",
    "        p_left = len(left_labels) / len(parent_labels)             # Calculate the proportion of samples in the left subset.\n",
    "        p_right = len(right_labels) / len(parent_labels)           # Calculate the proportion of samples in the right subset.\n",
    "        \n",
    "         # Calculate the information gain.\n",
    "        infogain = self.entropy(parent_labels) - (p_left * self.entropy(left_labels) + p_right * self.entropy(right_labels))\n",
    "        return infogain\n",
    "\n",
    "    def predict(self,X):\n",
    "        \"\"\"Make predictions for a set of instances.\"\"\"\n",
    "        \n",
    "        return [self.predict_instance(x,self.root) for x in X]  # Make predictions for each instance.\n",
    "\n",
    "    def predict_instance(self,x, node):\n",
    "        \"\"\"Recursively traverse the decision tree to make a prediction for a single instance.\"\"\"\n",
    "        \n",
    "        if (node.value != None):                          # Check if the current node is a leaf node.\n",
    "            return node.value\n",
    "\n",
    "        if (x[node.feature_index] <= node.threshold):       # Check if the instance should go to the left subtree.\n",
    "            return self.predict_instance(x,node.left)       # Recursively traverse the left subtree.\n",
    "        else:\n",
    "            return self.predict_instance(x,node.right)      # Recursively traverse the left subtree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a738a62-3a0c-4fa1-bb28-55634037017f",
   "metadata": {},
   "source": [
    "**For the Iris dataset, calculating the accuracy using 10 fold cross-validation for each value of min.(n_min E {5, 10, 15, 20})**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb7a4525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_iris(X, y):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Define the values for min_samples_split\n",
    "    n_min_values = [5, 10, 15, 20]\n",
    "    \n",
    "    # Number of splits for KFold\n",
    "    num_splits = 10\n",
    "    \n",
    "    # Initialize KFold with specified number of splits\n",
    "    kf = KFold(n_splits=num_splits)\n",
    "    \n",
    "    # Create a matrix to store results (accuracy scores)\n",
    "    result_matrix = np.zeros((len(n_min_values), num_splits))\n",
    "    \n",
    "    # Iterate over each min_samples_split value\n",
    "    for i, n_min in enumerate(n_min_values):\n",
    "        \n",
    "        # Iterate over each split in KFold\n",
    "        for index, (train_index, _) in enumerate(kf.split(X_train, y_train)):\n",
    "            \n",
    "            # Get the training data for the current split\n",
    "            x_train_fold, y_train_fold = X_train[train_index], y_train[train_index]\n",
    "            \n",
    "            # Initialize a decision tree classifier with current min_samples_split value\n",
    "            decision_tree = DecisionTree(min_samples_split=n_min / 100 * len(x_train_fold))\n",
    "            \n",
    "            # Fit the classifier on the training data\n",
    "            decision_tree.fit(x_train_fold, y_train_fold)\n",
    "            \n",
    "            # Make predictions on the test set\n",
    "            y_pred = decision_tree.predict(X_test)\n",
    "            \n",
    "            # Calculate and store the accuracy score for the current split\n",
    "            result_matrix[i][index] = accuracy_score(list(y_test.reshape(len(y_pred),)), y_pred)\n",
    "        \n",
    "        # Calculate average accuracy and standard deviation for the current min_samples_split value\n",
    "        avg_accuracy = np.mean(result_matrix[i, :])\n",
    "        avg_std_dev = np.std(result_matrix[i, :])\n",
    "        \n",
    "        # Print the average accuracy and standard deviation for the current min_samples_split value\n",
    "        print(\"For n_min =\", n_min, \":\\n\", \n",
    "              \"average accuracy:\", avg_accuracy, \"\\n\",\n",
    "              \"average standard deviation:\", avg_std_dev, \"\\n\")\n",
    "\n",
    "    # Print results in a table\n",
    "    headers = [\"n_min\", \"Average Accuracy\", \"Average Standard Deviation\"]\n",
    "    data = [[n_min_values[i], np.mean(result_matrix[i, :]), np.std(result_matrix[i, :])] for i in range(len(n_min_values))]\n",
    "    print(tabulate(data, headers = headers, tablefmt = \"grid\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ced19d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_min = 5 :\n",
      " average accuracy: 0.97 \n",
      " average standard deviation: 0.03480102169636849 \n",
      "\n",
      "For n_min = 10 :\n",
      " average accuracy: 0.9666666666666666 \n",
      " average standard deviation: 0.02108185106778919 \n",
      "\n",
      "For n_min = 15 :\n",
      " average accuracy: 0.9733333333333334 \n",
      " average standard deviation: 0.019999999999999997 \n",
      "\n",
      "For n_min = 20 :\n",
      " average accuracy: 0.9733333333333334 \n",
      " average standard deviation: 0.019999999999999997 \n",
      "\n",
      "+---------+--------------------+------------------------------+\n",
      "|   n_min |   Average Accuracy |   Average Standard Deviation |\n",
      "+=========+====================+==============================+\n",
      "|       5 |           0.97     |                    0.034801  |\n",
      "+---------+--------------------+------------------------------+\n",
      "|      10 |           0.966667 |                    0.0210819 |\n",
      "+---------+--------------------+------------------------------+\n",
      "|      15 |           0.973333 |                    0.02      |\n",
      "+---------+--------------------+------------------------------+\n",
      "|      20 |           0.973333 |                    0.02      |\n",
      "+---------+--------------------+------------------------------+\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./iris.csv\", names = ['Length_Sepal', 'Width_Sepal', 'Length_Petal','Width_Petal', 'Target'])\n",
    "df = df.replace({'Iris-setosa':0, 'Iris-versicolor': 1, 'Iris-virginica': 2})\n",
    "\n",
    "\n",
    "X = df.iloc[:,:-1].values\n",
    "y = df.iloc[:,-1].values.reshape(-1,1)\n",
    "cross_validate_iris(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e9b8a-e9f1-4c91-a7af-3a3c7d84def7",
   "metadata": {},
   "source": [
    "**For the Spambase dataset, calculating the accuracy using 10 fold cross-validation for each value of min (n_min E {5, 10, 15, 20, 25})**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdf73f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_spambase(X, y):\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32)\n",
    "    \n",
    "    # Define the values for min_samples_split\n",
    "    n_min_values = [5, 10, 15, 20, 25]\n",
    "    \n",
    "    # Number of splits for KFold\n",
    "    num_splits = 10\n",
    "    \n",
    "    # Initialize KFold with specified number of splits\n",
    "    kf = KFold(n_splits=num_splits)\n",
    "    \n",
    "    # Create a matrix to store results (accuracy scores)\n",
    "    result_matrix = np.zeros((len(n_min_values), num_splits))\n",
    "    \n",
    "    # Iterate over each min_samples_split value\n",
    "    for i, n_min in enumerate(n_min_values):\n",
    "        \n",
    "        # Iterate over each split in KFold\n",
    "        for index, (train_index, _) in enumerate(kf.split(X_train, y_train)):\n",
    "            \n",
    "            # Get the training data for the current split\n",
    "            x_train_fold, y_train_fold = X_train[train_index], y_train[train_index]\n",
    "            \n",
    "            # Initialize a decision tree classifier with current min_samples_split value\n",
    "            my_classifier_model = DecisionTree(min_samples_split = n_min / 100 * len(x_train_fold))\n",
    "            \n",
    "            # Fit the classifier on the training data\n",
    "            my_classifier_model.fit(x_train_fold, y_train_fold)\n",
    "            \n",
    "            # Make predictions on the test set\n",
    "            y_pred = my_classifier_model.predict(X_test)\n",
    "            \n",
    "            # Calculate and store the accuracy score for the current split\n",
    "            result_matrix[i][index] = accuracy_score(list(y_test.reshape(len(y_pred),)), y_pred)\n",
    "        \n",
    "        # Calculate average accuracy and standard deviation for the current min_samples_split value\n",
    "        avg_accuracy = np.mean(result_matrix[i, :])\n",
    "        avg_std_dev = np.std(result_matrix[i, :])\n",
    "        \n",
    "        # Print the average accuracy and standard deviation for the current min_samples_split value\n",
    "        print(\"For n_min =\", n_min, \":\\n\", \n",
    "              \"average accuracy:\", avg_accuracy, \"\\n\",\n",
    "              \"average standard deviation:\", avg_std_dev, \"\\n\")\n",
    "\n",
    "    # Print results in a table\n",
    "    headers = [\"n_min\", \"Average Accuracy\", \"Average Standard Deviation\"]\n",
    "    data = [[n_min_values[i], np.mean(result_matrix[i, :]), np.std(result_matrix[i, :])] for i in range(len(n_min_values))]\n",
    "    print(tabulate(data, headers=headers, tablefmt=\"grid\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d21b948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_min = 5 :\n",
      " average accuracy: 0.9044565217391305 \n",
      " average standard deviation: 0.009848209993476704 \n",
      "\n",
      "For n_min = 10 :\n",
      " average accuracy: 0.8891304347826086 \n",
      " average standard deviation: 0.002956841414942479 \n",
      "\n",
      "For n_min = 15 :\n",
      " average accuracy: 0.8653260869565218 \n",
      " average standard deviation: 0.012000413508943488 \n",
      "\n",
      "For n_min = 20 :\n",
      " average accuracy: 0.8593478260869565 \n",
      " average standard deviation: 0.0034441259821206076 \n",
      "\n",
      "For n_min = 25 :\n",
      " average accuracy: 0.8460869565217392 \n",
      " average standard deviation: 0.013144536050287784 \n",
      "\n",
      "+---------+--------------------+------------------------------+\n",
      "|   n_min |   Average Accuracy |   Average Standard Deviation |\n",
      "+=========+====================+==============================+\n",
      "|       5 |           0.904457 |                   0.00984821 |\n",
      "+---------+--------------------+------------------------------+\n",
      "|      10 |           0.88913  |                   0.00295684 |\n",
      "+---------+--------------------+------------------------------+\n",
      "|      15 |           0.865326 |                   0.0120004  |\n",
      "+---------+--------------------+------------------------------+\n",
      "|      20 |           0.859348 |                   0.00344413 |\n",
      "+---------+--------------------+------------------------------+\n",
      "|      25 |           0.846087 |                   0.0131445  |\n",
      "+---------+--------------------+------------------------------+\n"
     ]
    }
   ],
   "source": [
    "df_spambase = pd.read_csv(\"./spambase.csv\")\n",
    "\n",
    "\n",
    "X = df_spambase.iloc[:,:-1].values\n",
    "y = df_spambase.iloc[:,-1].values.reshape(-1,1)\n",
    "\n",
    "\n",
    "cross_validate_spambase(X,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
