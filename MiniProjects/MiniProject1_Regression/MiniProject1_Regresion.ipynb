{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99f49bbb",
   "metadata": {},
   "source": [
    "**1.**\n",
    "\n",
    "**Importing necessary libraries and exploring the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae3d3a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "## Avoid printing out warnings\n",
    "with warnings.catch_warnings():\n",
    "     warnings.filterwarnings(\"ignore\")\n",
    "     X, y = load_boston(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694da0ba",
   "metadata": {},
   "source": [
    "**Exploring the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4e7945d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and column in the data: (506, 13)\n",
      "Dimension of label data: (506,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows and column in the data:\", X.shape)\n",
    "print(\"Dimension of label data:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb508075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset:\n",
      " [[6.3200e-03 1.8000e+01 2.3100e+00 ... 1.5300e+01 3.9690e+02 4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9690e+02 9.1400e+00]\n",
      " [2.7290e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9283e+02 4.0300e+00]\n",
      " ...\n",
      " [6.0760e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 5.6400e+00]\n",
      " [1.0959e-01 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9345e+02 6.4800e+00]\n",
      " [4.7410e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 7.8800e+00]]\n",
      "\n",
      " The labeled data:\n",
      " [24.  21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 15.  18.9 21.7 20.4\n",
      " 18.2 19.9 23.1 17.5 20.2 18.2 13.6 19.6 15.2 14.5 15.6 13.9 16.6 14.8\n",
      " 18.4 21.  12.7 14.5 13.2 13.1 13.5 18.9 20.  21.  24.7 30.8 34.9 26.6\n",
      " 25.3 24.7 21.2 19.3 20.  16.6 14.4 19.4 19.7 20.5 25.  23.4 18.9 35.4\n",
      " 24.7 31.6 23.3 19.6 18.7 16.  22.2 25.  33.  23.5 19.4 22.  17.4 20.9\n",
      " 24.2 21.7 22.8 23.4 24.1 21.4 20.  20.8 21.2 20.3 28.  23.9 24.8 22.9\n",
      " 23.9 26.6 22.5 22.2 23.6 28.7 22.6 22.  22.9 25.  20.6 28.4 21.4 38.7\n",
      " 43.8 33.2 27.5 26.5 18.6 19.3 20.1 19.5 19.5 20.4 19.8 19.4 21.7 22.8\n",
      " 18.8 18.7 18.5 18.3 21.2 19.2 20.4 19.3 22.  20.3 20.5 17.3 18.8 21.4\n",
      " 15.7 16.2 18.  14.3 19.2 19.6 23.  18.4 15.6 18.1 17.4 17.1 13.3 17.8\n",
      " 14.  14.4 13.4 15.6 11.8 13.8 15.6 14.6 17.8 15.4 21.5 19.6 15.3 19.4\n",
      " 17.  15.6 13.1 41.3 24.3 23.3 27.  50.  50.  50.  22.7 25.  50.  23.8\n",
      " 23.8 22.3 17.4 19.1 23.1 23.6 22.6 29.4 23.2 24.6 29.9 37.2 39.8 36.2\n",
      " 37.9 32.5 26.4 29.6 50.  32.  29.8 34.9 37.  30.5 36.4 31.1 29.1 50.\n",
      " 33.3 30.3 34.6 34.9 32.9 24.1 42.3 48.5 50.  22.6 24.4 22.5 24.4 20.\n",
      " 21.7 19.3 22.4 28.1 23.7 25.  23.3 28.7 21.5 23.  26.7 21.7 27.5 30.1\n",
      " 44.8 50.  37.6 31.6 46.7 31.5 24.3 31.7 41.7 48.3 29.  24.  25.1 31.5\n",
      " 23.7 23.3 22.  20.1 22.2 23.7 17.6 18.5 24.3 20.5 24.5 26.2 24.4 24.8\n",
      " 29.6 42.8 21.9 20.9 44.  50.  36.  30.1 33.8 43.1 48.8 31.  36.5 22.8\n",
      " 30.7 50.  43.5 20.7 21.1 25.2 24.4 35.2 32.4 32.  33.2 33.1 29.1 35.1\n",
      " 45.4 35.4 46.  50.  32.2 22.  20.1 23.2 22.3 24.8 28.5 37.3 27.9 23.9\n",
      " 21.7 28.6 27.1 20.3 22.5 29.  24.8 22.  26.4 33.1 36.1 28.4 33.4 28.2\n",
      " 22.8 20.3 16.1 22.1 19.4 21.6 23.8 16.2 17.8 19.8 23.1 21.  23.8 23.1\n",
      " 20.4 18.5 25.  24.6 23.  22.2 19.3 22.6 19.8 17.1 19.4 22.2 20.7 21.1\n",
      " 19.5 18.5 20.6 19.  18.7 32.7 16.5 23.9 31.2 17.5 17.2 23.1 24.5 26.6\n",
      " 22.9 24.1 18.6 30.1 18.2 20.6 17.8 21.7 22.7 22.6 25.  19.9 20.8 16.8\n",
      " 21.9 27.5 21.9 23.1 50.  50.  50.  50.  50.  13.8 13.8 15.  13.9 13.3\n",
      " 13.1 10.2 10.4 10.9 11.3 12.3  8.8  7.2 10.5  7.4 10.2 11.5 15.1 23.2\n",
      "  9.7 13.8 12.7 13.1 12.5  8.5  5.   6.3  5.6  7.2 12.1  8.3  8.5  5.\n",
      " 11.9 27.9 17.2 27.5 15.  17.2 17.9 16.3  7.   7.2  7.5 10.4  8.8  8.4\n",
      " 16.7 14.2 20.8 13.4 11.7  8.3 10.2 10.9 11.   9.5 14.5 14.1 16.1 14.3\n",
      " 11.7 13.4  9.6  8.7  8.4 12.8 10.5 17.1 18.4 15.4 10.8 11.8 14.9 12.6\n",
      " 14.1 13.  13.4 15.2 16.1 17.8 14.9 14.1 12.7 13.5 14.9 20.  16.4 17.7\n",
      " 19.5 20.2 21.4 19.9 19.  19.1 19.1 20.1 19.9 19.6 23.2 29.8 13.8 13.3\n",
      " 16.7 12.  14.6 21.4 23.  23.7 25.  21.8 20.6 21.2 19.1 20.6 15.2  7.\n",
      "  8.1 13.6 20.1 21.8 24.5 23.1 19.7 18.3 21.2 17.5 16.8 22.4 20.6 23.9\n",
      " 22.  11.9]\n"
     ]
    }
   ],
   "source": [
    "print(\"The dataset:\\n\",X )\n",
    "print(\"\\n The labeled data:\\n\",y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759598c2",
   "metadata": {},
   "source": [
    "**Making new X since we also need the constant term in the equation, so we add 1s column to left side of the dataset.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "228e2277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = np.c_[np.ones((X.shape[0],1)),X]\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69a80616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to normalize features\n",
    "def normalize_features(X):\n",
    "    mean = np.mean(X, axis=0)\n",
    "    \n",
    "    std = np.std(X, axis=0)\n",
    "    \n",
    "    normalized_X = (X - mean) / std\n",
    "    \n",
    "    return normalized_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73705389",
   "metadata": {},
   "source": [
    "**3.** \n",
    "\n",
    "**Linear Regression**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d49a880",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to fit LR using closed form solution\n",
    "def weight_function(X,y):\n",
    "    weight = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    return weight\n",
    "    \n",
    "# Function to compute mean squared error\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "#Function to compute predicted values\n",
    "def predicted_values(X, weight):\n",
    "    return X @ weight\n",
    "\n",
    "#K-Cross Validation for LR\n",
    "def k_fold_cross_validation(X, y, k, weight_function):\n",
    "    \n",
    "    kf = KFold(n_splits=k, shuffle=True,random_state=42)\n",
    "    \n",
    "    mse_scores_test_set = []\n",
    "    mse_scores_train_set = []\n",
    "    \n",
    "    for train, test in kf.split(X_new):\n",
    "        X_train, X_test = X_new[train], X_new[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "\n",
    "        weight = weight_function(X_train, y_train)\n",
    "\n",
    "        y_train_pred = predicted_values(X_train, weight)\n",
    "        y_test_pred = predicted_values(X_test, weight)\n",
    "\n",
    "        mse_scores_train_set.append(mean_squared_error(y_train, y_train_pred))\n",
    "        mse_scores_test_set.append(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    return np.mean(mse_scores_test_set), np.mean(mse_scores_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cc75831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Mean Square Error for Test Set, Mean Square Error for Train Set):"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(23.364203007531508, 21.818586996144017)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"(Mean Square Error for Test Set, Mean Square Error for Train Set):\",end=\"\")\n",
    "k_fold_cross_validation(X_new,y,10,weight_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738940f6",
   "metadata": {},
   "source": [
    "**4.**\n",
    "\n",
    "**Ridge regularization model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee88c1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closed form solution function for ridge regression\n",
    "def ridge_weight_function(X,y,lambdaValue):\n",
    "    X_t_and_X = X.T @ X\n",
    "    \n",
    "    A=np.eye(X_t_and_X.shape[1]) \n",
    "    A[0,0]=0                    #setting the top left of the identity as 0\n",
    "    \n",
    "    weight = np.linalg.inv(X_t_and_X + lambdaValue*A)@ X.T @ y\n",
    "    return weight\n",
    "\n",
    "#MSE function as before\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "#Prediction Function as before\n",
    "def predicted_values(X, weight):\n",
    "    return X @ weight\n",
    "\n",
    "#K-Cross_validation for Ridge Regression\n",
    "def ridge_k_fold_cross_validation(X, y, k, ridge_weight_function, lambdaValue):\n",
    "    kf = KFold(n_splits=k, shuffle=True,random_state=42)\n",
    "    \n",
    "    mse_scores_test_set = []\n",
    "    mse_scores_train_set = []\n",
    "    \n",
    "    for train, test in kf.split(X):\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "\n",
    "        weight = ridge_weight_function(X_train, y_train, lambdaValue)\n",
    "\n",
    "        y_train_pred = predicted_values(X_train, weight)\n",
    "        y_test_pred = predicted_values(X_test, weight)\n",
    "\n",
    "        mse_scores_train_set.append(mean_squared_error(y_train, y_train_pred))\n",
    "        mse_scores_test_set.append(mean_squared_error(y_test, y_test_pred))\n",
    "    \n",
    "    return np.mean(mse_scores_test_set), np.mean(mse_scores_train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9b59b7",
   "metadata": {},
   "source": [
    "**5**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e05fbef",
   "metadata": {},
   "source": [
    "**Finding best lambda**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c416a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 10.0\n",
      "(Mean Square Error for Test Set, Mean Square Error for Train Set):\n",
      "24.151167028806483, 22.609088327776966\n",
      "\n",
      "Lambda: 31.622776601683793\n",
      "(Mean Square Error for Test Set, Mean Square Error for Train Set):\n",
      "24.432328644123732, 22.98794193527157\n",
      "\n",
      "Lambda: 100.0\n",
      "(Mean Square Error for Test Set, Mean Square Error for Train Set):\n",
      "25.20417072442644, 23.860963584131078\n",
      "\n",
      "Lambda: 316.22776601683796\n",
      "(Mean Square Error for Test Set, Mean Square Error for Train Set):\n",
      "26.81563492602445, 25.5229860528301\n",
      "\n",
      "Lambda: 1000.0\n",
      "(Mean Square Error for Test Set, Mean Square Error for Train Set):\n",
      "29.109564651842412, 27.862974452706975\n",
      "\n",
      "Lambda: 3162.2776601683795\n",
      "(Mean Square Error for Test Set, Mean Square Error for Train Set):\n",
      "32.54415106038437, 31.37419291420291\n",
      "\n",
      "Lambda: 10000.0\n",
      "(Mean Square Error for Test Set, Mean Square Error for Train Set):\n",
      "38.47163995070741, 37.37495995816031\n",
      "\n",
      "Lambda: 31622.776601683792\n",
      "(Mean Square Error for Test Set, Mean Square Error for Train Set):\n",
      "47.36787615414065, 46.32456498852266\n",
      "\n",
      "Lambda: 100000.0\n",
      "(Mean Square Error for Test Set, Mean Square Error for Train Set):\n",
      "54.97008953923603, 54.05291525046478\n",
      "\n",
      "Lambda: 316227.7660168379\n",
      "(Mean Square Error for Test Set, Mean Square Error for Train Set):\n",
      "59.33155220804601, 58.586776754266246\n",
      "\n",
      "Lambda: 1000000.0\n",
      "(Mean Square Error for Test Set, Mean Square Error for Train Set):\n",
      "62.048189934355136, 61.44415410642456\n",
      "\n",
      "Lambda: 3162277.6601683795\n",
      "(Mean Square Error for Test Set, Mean Square Error for Train Set):\n",
      "64.19776828936418, 63.6906171884361\n",
      "\n",
      "Lambda: 10000000.0\n",
      "(Mean Square Error for Test Set, Mean Square Error for Train Set):\n",
      "67.73105020423333, 67.30287783660314\n",
      "\n",
      "Best Lambda: 10.0 with MSE: 24.151167028806483\n"
     ]
    }
   ],
   "source": [
    "#Function to print lambdas and find best lambda\n",
    "def best_lambda(X, y, lambdas, k=10):\n",
    "\n",
    "    best_lambda = None\n",
    "    best_test_mse = float('inf')\n",
    "\n",
    "    for lamb in lambdas:\n",
    "        print(\"Lambda:\",lamb)\n",
    "        print(\"(Mean Square Error for Test Set, Mean Square Error for Train Set):\",end=\"\\n\")\n",
    "        test_mse, train_mse = ridge_k_fold_cross_validation(X_new,y,10,ridge_weight_function,lamb)\n",
    "        print(f\"{test_mse}, {train_mse}\")\n",
    "\n",
    "        if test_mse < best_test_mse:\n",
    "            best_test_mse = test_mse\n",
    "            best_lambda = lamb\n",
    "            \n",
    "        print()\n",
    "        \n",
    "    print(\"Best Lambda:\", best_lambda, \"with MSE:\", best_test_mse)\n",
    "    return best_lambda, best_test_mse\n",
    "\n",
    "# Lambda values\n",
    "lambdas = np.logspace(1, 7, num=13)\n",
    "\n",
    "best_lambda, best_test_mse = best_lambda(X, y, lambdas, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4b3056",
   "metadata": {},
   "source": [
    "**6.**\n",
    "\n",
    "**Polynomial Regression** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a52f3c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of new data : (506, 104)\n"
     ]
    }
   ],
   "source": [
    "polynomial = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "X_array = np.array(X)\n",
    "\n",
    "polynomial.fit(X_array)\n",
    "\n",
    "polynomial_features = polynomial.transform(X_array)\n",
    "\n",
    "print(\"Shape of new data :\", polynomial_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61230e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance with Best Lambda: 10.0\n",
      "Average test MSE: 13.894347259785476, Average training MSE: 6.771334220874536\n"
     ]
    }
   ],
   "source": [
    "X_polynomial = np.c_[np.ones((polynomial_features.shape[0],1)),polynomial_features]\n",
    "\n",
    "best_test_mse, best_train_mse = ridge_k_fold_cross_validation(X_polynomial,y,10,ridge_weight_function,best_lambda)\n",
    "\n",
    "print(\"Performance with Best Lambda:\", best_lambda)\n",
    "print(f\"Average test MSE: {best_test_mse}, Average training MSE: {best_train_mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3fbb93",
   "metadata": {},
   "source": [
    "**7.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830dd9b2",
   "metadata": {},
   "source": [
    "**Multivariate Regression using gradient descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c0cd6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to perform gradient descent for MVR\n",
    "def weight_function_gradientdescent(X, y, rate, iterations=100000):\n",
    "    getwts = np.random.randn(X.shape[1],)\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        gradient = (2/m) * X.T.dot(np.matmul(X,getwts)-y)\n",
    "        getwts = getwts - rate * gradient\n",
    "        \n",
    "    return getwts\n",
    "\n",
    "def gardientdescent_k_fold_cross_validation(X, y, k, weight_function_gradientdescent,learning_rate):\n",
    "    m, n = X.shape\n",
    "    \n",
    "    X_new = np.c_[np.ones((m, 1)), X]\n",
    "    \n",
    "    kf = KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "    \n",
    "    mse_scores_test_set = []\n",
    "    mse_scores_train_set = []\n",
    "    \n",
    "    for train, test in kf.split(X):        \n",
    "        X_train, X_test = X_new[train], X_new[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        \n",
    "        weight = weight_function_gradientdescent(X_new, y,learning_rate)\n",
    "        \n",
    "        y_train_pred = predicted_values(X_train, weight)\n",
    "        y_test_pred = predicted_values(X_test, weight)\n",
    "        \n",
    "        mse_scores_train_set.append(mean_squared_error(y_train, y_train_pred))\n",
    "        mse_scores_test_set.append(mean_squared_error(y_test, y_test_pred))\n",
    "        \n",
    "    return np.mean(mse_scores_test_set), np.mean(mse_scores_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64e25881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Mean Square Error for Test Set, Mean Square Error for Train Set):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(40.059119529357, 40.36672128230183)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"(Mean Square Error for Test Set, Mean Square Error for Train Set):\",end=\"\\n\")\n",
    "gardientdescent_k_fold_cross_validation(X,y,10,weight_function_gradientdescent,0.000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45004fc",
   "metadata": {},
   "source": [
    "**8.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdbbfba",
   "metadata": {},
   "source": [
    "**Lasso Regression with Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3112cbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error for Test Set: 77.98543188442451\n",
      "Mean Square Error for Train Set: 76.99179204471955\n"
     ]
    }
   ],
   "source": [
    "#Lasso Regression\n",
    "def lasso_regression_fit(X, y, alpha=0.1, max_iter=1000, tol=1e-4, learning_rate=0.001):\n",
    "    m, n = X.shape\n",
    "    beta = np.zeros(n)\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        prev_beta = np.copy(beta)\n",
    "        gradients = lasso_gradient(X, y, beta, alpha)\n",
    "        beta -= learning_rate * gradients\n",
    "        \n",
    "        if np.linalg.norm(beta - prev_beta) < tol:\n",
    "            break\n",
    "            \n",
    "    return beta\n",
    "\n",
    "def lasso_gradient(X, y, beta, alpha):\n",
    "    error = y - np.dot(X, beta)\n",
    "    lasso_gradient = -(2 / y.size) * X.T.dot(error) + alpha * np.sign(beta)\n",
    "    return lasso_gradient\n",
    "\n",
    "def lasso_regression_predict(X, beta):\n",
    "    return np.dot(X, beta)\n",
    "\n",
    "def k_fold_cross_validation(X, y, k, alpha, learning_rate):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    mse_scores_for_test_set = []\n",
    "    mse_scores_for_train_set = []\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        beta = lasso_regression_fit(X_train, y_train, alpha=alpha, learning_rate=learning_rate)\n",
    "        y_train_pred = lasso_regression_predict(X_train, beta)\n",
    "        y_test_pred = lasso_regression_predict(X_test, beta)\n",
    "        \n",
    "        mse_train = np.mean((y_train - y_train_pred) ** 2)\n",
    "        mse_test = np.mean((y_test - y_test_pred) ** 2)\n",
    "        \n",
    "        mse_scores_for_train_set.append(mse_train)\n",
    "        mse_scores_for_test_set.append(mse_test)\n",
    "    \n",
    "    return np.mean(mse_scores_for_test_set), np.mean(mse_scores_for_train_set)\n",
    "\n",
    "alpha = 0.1\n",
    "learning_rate = 0.000001\n",
    "k = 10\n",
    "\n",
    "# Initialize and train Lasso regression model\n",
    "mse_test, mse_train = k_fold_cross_validation(X_new, y, k, alpha, learning_rate)\n",
    "\n",
    "print(\"Mean Square Error for Test Set:\", mse_test)\n",
    "print(\"Mean Square Error for Train Set:\", mse_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab7359d",
   "metadata": {},
   "source": [
    "**9.**\n",
    "\n",
    "**Elastic Net Regression with gradient descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18d59591",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error for Test Set: 73.50806976253273\n",
      "Mean Square Error for Train Set: 72.4778524881955\n"
     ]
    }
   ],
   "source": [
    "#Elastic Net Regression\n",
    "def elastic_net_mse(y, y_predicted, weight, alpha1, alpha2):\n",
    "    error = y - y_predicted\n",
    "    \n",
    "    l1_penalty = alpha1 * np.sum(np.abs(weight))\n",
    "    \n",
    "    l2_penalty = alpha2 * np.dot(weight, weight)\n",
    "    \n",
    "    return np.mean(error ** 2) + l1_penalty + l2_penalty\n",
    "\n",
    "def elastic_net_gradient(X, y, y_pred, weight, alpha1, alpha2):\n",
    "    error = y - y_pred\n",
    "    \n",
    "    l1_penalty_grad = alpha1 * np.sign(weight)\n",
    "    \n",
    "    l1_penalty_grad[0] = 0                                        # Exclude intercept term from L1 penalty gradient\n",
    "    l2_penalty_grad = 2 * alpha2 * weight\n",
    "    \n",
    "    return -(2 / len(y)) * X.T.dot(error) + l1_penalty_grad + l2_penalty_grad\n",
    "\n",
    "def elastic_net_gd(X, y, alpha1, alpha2, learning_rate, iterations=1000):\n",
    "    weight = np.zeros(X.shape[1])\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        y_pred = np.dot(X, weight)\n",
    "        \n",
    "        gradient = elastic_net_gradient(X, y, y_pred, weight, alpha1, alpha2)\n",
    "        weight -= learning_rate * gradient\n",
    "        \n",
    "    return weight\n",
    "\n",
    "def k_fold_cross_validation_elastic_net(X, y, alpha1, alpha2, learning_rate, k=10, iterations=1000):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    \n",
    "    mse_scores_test_set = []\n",
    "    mse_scores_train_set = []\n",
    "    \n",
    "    for train, test in kf.split(X):\n",
    "        \n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y[train], y[test]\n",
    "        \n",
    "        weight = elastic_net_gd(X_train, y_train, alpha1, alpha2, learning_rate, iterations)\n",
    "        \n",
    "        y_train_pred = np.dot(X_train, weight)\n",
    "        y_test_pred = np.dot(X_test, weight)\n",
    "        \n",
    "        mse_train = elastic_net_mse(y_train, y_train_pred, weight, alpha1, alpha2)\n",
    "        mse_test = elastic_net_mse(y_test, y_test_pred, weight, alpha1, alpha2)\n",
    "        \n",
    "        mse_scores_train_set.append(mse_train)\n",
    "        mse_scores_test_set.append(mse_test)\n",
    "        \n",
    "    return np.mean(mse_scores_test_set), np.mean(mse_scores_train_set)\n",
    "\n",
    "alpha1 = 0.1\n",
    "alpha2 = 0.2\n",
    "learning_rate = 0.000001\n",
    "k = 10\n",
    "iterations = 1000\n",
    "\n",
    "mse_test, mse_train = k_fold_cross_validation_elastic_net(X_new, y, alpha1, alpha2, learning_rate, k, iterations)\n",
    "print(\"Mean Square Error for Test Set:\", mse_test)\n",
    "print(\"Mean Square Error for Train Set:\", mse_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070a9ca5",
   "metadata": {},
   "source": [
    "**10.**\n",
    "\n",
    "*Given the option to predict future housing prices using one model, I would opt for polynomial transformation with ridge regression (referencing Question No. 6). This choice is based on its impressive performance, as evidenced by its lowest test mean squared error of 13.89. This suggests that the model exhibits low variance when applied to test data. Additionally, the average training mean squared error of 6.77 indicates that the model is not overly complex or prone to overfitting, meaning its bias is also low.\n",
    "\n",
    "The current performance metrics are as follows:\n",
    "\n",
    "Performance after polynomial transformation with the best Lambda (10.00):\n",
    "\n",
    "-Average training MSE: 6.771\n",
    "-Average test MSE: 13.894\n",
    "\n",
    "These metrics were obtained using ridge regression (closed form) with the following parameters:\n",
    "\n",
    "-Training dataset X, y and prediction dataset X(i)\n",
    "-Regularization parameter (Lambda) set to 10\n",
    "-Polynomial of degree 2\n",
    "-Mean Squared Error (MSE) as the evaluation metric*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
